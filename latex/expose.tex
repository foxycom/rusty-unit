% Notes on the writing of the Master Thesis
% Book: How to Write a Lot, Paul Silvia, 2nd Edition
% Book: Getting Things Done, David Allen
%
%
% Mögliche Abgrenzung von anderen: Den hybriden Ansatz von SBST mit DSE verwenden, 
% so wie im Paper von Baars et al.
%
% Mögliche weitere Evaluation: Vergleiche die Coverage von generierten Tests zu der 
% Coverage von manuell geschriebenen Tests von Entwicklern in evaluierten Programmen.
% 

\documentclass{article}
% Please do not change this options...
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{todonotes}
\usepackage{graphicx}
\graphicspath{ {./img/} }
\usepackage{pgfgantt}
\newcounter{myWeekNum}
\stepcounter{myWeekNum}
%
\newcommand{\myWeek}{\themyWeekNum
    \stepcounter{myWeekNum}
    \ifnum\themyWeekNum=53
         \setcounter{myWeekNum}{1}
    \else\fi
}


\begin{document}
\setcounter{myWeekNum}{25}
\ganttset{%
calendar week text={\myWeek{}}%
}


\title{Exposé: Irgendwas mit Rust}
\author{Vsevolod Tymofyeyev}
\date{\today}
\maketitle

\section{Motivation / Problembeschreibung}
%\begin{itemize}
%    \item Darstellung des Themas der Masterarbeit
%    \item Begründung/Motivation
%    \item Relevanz
%\end{itemize}

In der Programiersprachenwelt, in der es zwei große Fronten gibt (low-level Sprachen, die auf Kosten von Sicherheit mehr Performanz bieten und high-level Sprachen, die durch bestimmte Konstrukte wie Garbage-Collector Sicherheiten für Programmierer bieten, die jedoch zu Laufzeit-Overhead führen) versucht Rust beides zu verbinden. Die statisch typisierte Sprache für Systemprogrammierung verspricht eine ähnlich hohe Performanz wie C++ mit erweiterter Typ- und Speichersicherheit by default. Invarianten werden zur Kompilierzeit sichergestellt, wodurch Abstraktionen (sogenannte Zero-Cost-Abstractions) und automatische Speicherverwaltung mit keinen Laufzeitkosten verbunden sind, wie es zum Beispiel bei Sprachen mit Garbage Collection der Fall ist. Rust verhindert unter anderem folgende oft verbreitete Probleme: 
\begin{itemize}
    \item Dangling pointers
    \item Data races
    \item Integer overflow
    \item Buffer overflow
    \item Iterator invalidation
\end{itemize}
Nur die Integer und  Buffer overflows werden zur Laufzeit überprüft, wobei die Buffer overflows durch das Benutzen von Iteratoren auf statische Checks reduziert werden können~\cite{Anderson2016}. Diese Symbiose führte dazu, dass die Sprache besonders attraktiv auf Entwickler wirkt, wodurch sie trotz ihres sehr jungen Geschichte bereits seit mehreren Jahren die Beliebtheitsrankings stürmt~\cite{StackOverflow2020}. Selbst Spitzenkonzerne erwägen die Anwendung von Rust in Teilen ihrer Software. Laut Microsoft und Google sind 70\% der in ihrer Software in den vergangenen Jahren gefundenen Fehler auf Speicherlecks zurückzuführen, hervorgerufen durch die weitverwendeten unsicheren Sprachen wie C und C++~\cite{Microsoft2019MemoryBugs, RustInAndroid}. Microsoft, SpaceX, Google, Amazon AWS und viele andere Unternehmen fingen bereits an, Rust in ihren Produkten zur erhöhten Sicherheit zu verwenden~\cite{MicrosoftJoinsRust, AmazonLovesRust, RustInAndroid, GoogleRustFoundation}.

Nichtsdestotrotz, kann auch der Rust Compiler nicht die komplette Korrektheit eines Programms garantieren, wodurch auch bei der Programmierung mit dieser Sprache das Testen der geschriebenen Software einem nicht erspart bleibt. Software Testen erfodert Daten, deren manuelle Selektion die Aufgabe eines Programmierers ist. Dieses Vorgehen ist aber in der Regel sehr aufwändig und kostenintensiv. Eine genügend komplexe Software kann Tausende Ausführungspfade haben, die durch verschiedene Inputdaten angesteuert werden und von einem Menschen unter Umständen übersehen werden können, schließlich müssten fast genauso viele Tests geschrieben werden. Ein weiterer Punkt ist, dass sich Software-Anforderungen mit der Zeit ändern können, was dazu führt, dass existierende Testsuites dadurch eventuell manuell verändert bzw. im schlimmsten Fall neu geschrieben werden müssen. Somit ist das Abdecken von allen möglichen Ausführungsfällen oder gar eine exhaustive Coverage schlicht wirtschaftlich und menschlich kaum zu leisten~\cite{Myers2012}. Es wird angenommen, dass ungefähr die Hälfte des Budgets in Software Projekten für das Testen ausgegeben wird. Es ist also nicht überraschend, dass ungefähr derselbe Anteil von SBSE Artikel ausgerechnet SBST behandelt~\cite{Harman2015}. Außerdem, trotz der ausgereiften Testing Tools, stehen Entwickler oft unter Zeitdruck (z. B. Deadlines bei Projekten) und haben nicht genug Zeit, die immer komplexer werdende Software zu testen. Das ist ein großes Problem, denn auch wenn einige kleinen Bugs nur zur Unzufriedenheit eines Endnutzers führen, können einige andere erhebliche wirtschaftliche und selbst gesundheitliche Schäden auslösen~\cite{Myers2012}. \todo{Ein paar Beispiele für krasse Vorfälle wegen Software Bugs wären hier praktisch, z. B. Ariane V Explosion} Aus diesem Grund sind in den letzten Jahren bzw. Jahrzehnten viele Ansätze entstanden, um diesen Prozess zu automatisieren, indem Tests aus einer gegebenen Software generiert werden~\cite{McMinn_2004}. 

Das Generieren der Tests geschieht meistens, indem bestimmte Zielkriterien gesetzt werden, zum Beispiel die Codecoverage des getesteten Programms. Die Coveragekriterien sind eine endliche Sammlung von Zielen, die typischerweise einzeln abgearbeitet werden, wobei mittels symbolischer Ausführung oder mit einem such-basierten Ansatz die Inputdaten generiert werden, um ein Ziel zu erreichen bzw. abzudecken~\cite{Fraser_2013}.  

Da Rust als stabile Programmiersprache als jung gilt und im Jahr 2015 in der Version 1.0 erschien~\cite{Rust10}, gibt es zum Stand des Schreibens nur relativ wenige Optionen für eine automatische Testgenerierung. Diese beschränken sich auf Tools, die mittels Symbolic Execution die möglichen Pfade in einem gegebenen Programm durchsuchen~\cite{cadar2008klee}. \todo{Weitere Tools?} Außerdem benutzen die Tools die IR von LLVM, welches vom Rust Compiler eingesetzt wird. Zusätzlich bringt das affine Typ-System von Rust~\cite{Anderson2016} einige Hürden mit, verglichen zu Sprachen wie z. B. Java. Es gibt aber zum Stand des Schreibens keinen bekannten Einsatz von SBST für Rust. SBST ist eine Kombination aus automatischer Testgenerierung und metaheuristischen Suchtechniken. Diese Unterkategorie von SBSE greift zu Optimisierungsalgorithmen, um ein eigentlich NP-hartes Problem der Testgenerierung mit möglichst hoher Testabdeckung möglichst effizient und effektiv zu lösen~\cite{Khari2019}. SBST optimizes a solution as much as possible with respect to a certain objective, which could be test case priorization, test suite minimization, max out real-time properites of the SUT, and so on~\cite{Khari2019}. Die dadurch generierten Tests streben eine höhere Coverage an, um möglichst viele Fälle abzudecken. 


\section{Zielsetzung}
%\begin{itemize}
%    \item Ziel der Arbeit und Erkenntnisinteresse herausstellen
%    \item Ergebnisse skizzieren
%\end{itemize}
Das Ziel dieser Masterarbeit ist es, die Effektivität eines such-basierten Ansatzes zur Generierung von Tests in Form vom menschenlesbaren Quellcode zu evaluieren, um die Frage zu beantworten, wie hoch die Testabdeckung von solchen Tests ist (auch im Vergleich zu anderen Ansätzen).


\section{Forschungsstand}
\begin{itemize}
    \item EvoSuite
    \item KLEE
    \item MOSA
    \item DynaMOSA
    \item ... 
\end{itemize}
Das Generieren der Tests geschieht meistens, indem bestimmte Zielkriterien gesetzt werden, zum Beispiel die Codecoverage des getesteten Programms. Die Coveragekriterien sind eine endliche Sammlung von Zielen, die typischerweise einzeln abgearbeitet werden, wobei mittels symbolischer Ausführung oder mit einem such-basierten Ansatz die Inputdaten generiert werden, um ein Ziel zu erreichen bzw. abzudecken~\cite{Fraser_2013}. 

Symbolische Ausführung ist ein verbreiteter Ansatz, um Inputdaten oder ganze Unit-Tests zu generieren, indem Pfadeinschränkungen aufgelöst werden. Dynamische symbolische Ausführung ist eine Erweiterung von DS, die erlaubt, mit Hilfe einer Kombination aus konkreten und symbolischen Werten eine Reihe von Problemen zu überwinden~\cite{Fraser_2013}. Es gibt eine Reihe von Tools, die für eine automatische Generierung von Tests auf DSE setzen, zum Beispiel CUTE and jCUTE~\cite{10.1007/11817963_38} und KLEE~\cite{cadar2008klee}. \todo{Vielleicht gibt es jetzt neuere Tools. Außerdem besser ein anderes Paper zum Zitieren hernehmen, zum Beispiel das von KLEE oder eins, das Grundlagen von DSE beschreibt.}

Metaheuristische Suchtechniken sind eine Alternative zu DSE und haben ihre Anfänge in den 70er Jahren~\cite{McMinn_2004}. Damals wurden schon die Schlüsselkonzepte wie Branch Distance~\cite{Korel1990} und Approach Level~\cite{Wegener2001} definiert, um die Suche nach Inputdaten für die Tests in die richtige Richtung zu ''führen''.  

Der Traditionelle Ansatz in Such-basierten Algorithmen ist es, für jedes einzelne Objetive ein Test zu generieren, der dieses Objective durch gezielte Ausführung mit passenden Daten anstößt. Man versucht mit den generierten Tests so viel Code abzudecken, wie nur möglich, in dem zum Beispiel alle Branches ausgeführt werden (Branch Coverage). Ein großer Nachteil von dem Ansatz ist, dass Objectives oft voneinander abhängig und nicht immer feasible sind. Das heißt, die Effektivität einer Suche hängt stark von der Reihenfolge der Objectives ab~\cite{Fraser_2013}. Außerdem kann die Größe der generierten Testsuite stark variieren, da ein bestimmter Test zufällig mehrere Branches abdecken kann (eine sogennante kollaterale Abdeckung)~\cite{Harman2010}. Es ist allgemein schwierig, bei den generierten Tests automatische Testorakel zu erzeugen. Testorakel sind nötig, um zu überprüfen, ob das Verhalten des getesteten Programms richtig ist oder nicht. Der trivialste Testorakel ist ein Absturz des Programms. \todo{Testorakel Referenz} Man möchte allerdings auch anderes testen, zum Beispiel ob die Ausgabe einer Funktion korrekt ist. Das kann durch manuelles Ergänzen der Tests durch einen Menschen erreicht werden, allerdings müssten die generierten Tests kurz genug sein, um noch verständlich zu bleiben. Also ist nicht nur die Coverage ein wichtiges Kriterium bei der Generierung von Tests, sondern auch deren Länge~\cite{Fraser_2013}. 
\section{Forschungskonzept}
\begin{itemize}
    \item Forschungsfragen
    \item Hypothesen
    \item Method + Begründung
    \item Daten
    \item Evtl. benötigte Mittel
\end{itemize}

% Wenn man beispielsweise 40 Seiten schreiben möchte, dann wären 
% Solution Approach und Evaluation jeweils ~10 Seiten lang. Es ist okay, wenn
% Introduction dann ungefähr nur 2-3 Seiten lang ist.
\section{Vorläufige Gliederung}
\begin{enumerate}
    \item Introduction
    \item Background \begin{enumerate}
        \item Random Testing
        \item Single-objective Search-based techniques
        \item Multi-objective Search-based techniques
        \item Dynamic Symbolic Execution
    \end{enumerate}
    \item State of the Art \begin{enumerate}
        \item Fuzzer for Rust
        \item Test Generators for Rust
    \end{enumerate}
    \item Search-based Unit Test Generation in Rust (1/3 of time) \begin{enumerate}
        \item Testability Tranformations
        \item Instrumentation
        \item ...
    \end{enumerate}
    \item Evaluation (1/3 of time) \begin{enumerate} 
        \item Setup 
        \item Threats to Validity
        \item Code Coverage Comparison with Manually Written Tests
        \item Code Coverage Comparison with Other Tools
        \item (Number of found Bugs / Mutants?)
    \end{enumerate}
    \item Conclusion
    \item Future Work
    \item Appendix
    \item References
\end{enumerate}

%\section{Zeitplan}
%\noindent\resizebox{\textwidth}{!}{
%\begin{ganttchart}[
%    hgrid,
%    vgrid={*6{draw=none}, dotted},
%    time slot format=isodate,
%    y unit chart=1cm,
%    y unit title=1cm,
%    x unit=.1cm]{2021-6-21}{2021-12-24},
    
%\gantttitlecalendar{month=name, week=25} \\

%\gantttitle{2021}{12} \\
%\gantttitlelist{25,...,52}{1} \\
%\ganttgroup{Group 1}{1}{7} \\


%\ganttbar{Literaturrecherche}{2021-6-21}{2021-7-18} \\
%\ganttbar{Konzeptphase}{2021-7-12}{2021-8-1} \\
%\ganttbar{Implementierung}{2021-8-2}{2021-9-12} \\
%\ganttbar{Testen + Evaluation}{2021-9-13}{2021-10-17} \\
%\ganttbar{Schriftliche Ausarbeitung}{2021-10-4}{2021-11-21} \\
%\ganttbar{Korrekturlesen + Nachbessern}{2021-11-22}{2021-11-28} \\
%\ganttbar{Puffer}{2021-11-29}{2021-12-18} \\
%\ganttbar{Abgabe}{2021-12-18}{2021-12-19} \\


%\ganttlinkedbar{Task 2}{3}{7} \ganttnewline
%\ganttmilestone{Milestone}{7} \ganttnewline
%\ganttbar{Final Task}{8}{12}
%\ganttlink{elem2}{elem3}
%\ganttlink{elem3}{elem4}
%\end{ganttchart}
%}

%\newpage  % End of first page: you can remove this if you do not need it

%\section{Critical questions}
%\paragraph{Q1:} Is there a way to define some kind of coverage of the generated scenarios?
%\paragraph{Q2:} While some real accidents might be somewhat similar to each other, the %abstract police reports written in natural language boost the similarity of those further. How could the generated test scenarios be limited only to unique ones?

% Remember to list at not less than 4 related work, you can list more if you wish
% If you like you can also use bib files (see below)


\bibliographystyle{unsrt}
\bibliography{bibliography}
%\nocite*{}
\end{document}