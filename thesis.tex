% Notes on the writing of the Master Thesis
% Book: How to Write a Lot, Paul Silvia, 2nd Edition
% Book: Getting Things Done, David Allen
%
%
% Mögliche Abgrenzung von anderen: Den hybriden Ansatz von SBST mit DSE verwenden, 
% so wie im Paper von Baars et al.
%
% Mögliche weitere Evaluation: Vergleiche die Coverage von generierten Tests zu der 
% Coverage von manuell geschriebenen Tests von Entwicklern in evaluierten Programmen.
% 

\documentclass{article}
% Please do not change this options...
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{graphicx}
\graphicspath{ {./img/} }
\usepackage{todonotes}


\begin{document}

\title{Master Thesis}
\author{Vsevolod Tymofyeyev}
\date{\today}
\maketitle

\tableofcontents
\section{Introduction}
In der Programiersprachenwelt, in der es zwei große Fronten gibt (low-level Sprachen, die auf Kosten von Sicherheit mehr Performanz bieten und high-level Sprachen, die durch bestimmte Konstrukte wie Garbage-Collector Sicherheiten für Programmierer bieten, die jedoch zu Laufzeit-Overhead führen) versucht Rust beides zu verbinden. Die Sprache für Systemprogrammierung verspricht eine ähnlich hohe Performanz wie C++ mit erweiterter Typ- und Speichersicherheit. Invarianten werden zur Kompilierzeit sichergestellt, wodurch Abstraktionen mit keinen Laufzeitkosten verbunden sind (sogenannte Zero-Cost-Abstractions). Diese Symbiose führte dazu, dass die Sprache besonders attraktiv auf Entwickler wirkte, wodurch sie trotz ihres sehr jungen Geschichte bereits seit mehreren Jahren die Beliebtheitsrankings stürmt~\cite{StackOverflow2020}. Selbst Spitzenkonzerne erwägen eine Nutzung oder gar Umschreibung von Teilen ihrer Codebase nach Rust. Laut Microsoft und Google sind 70\% der in ihrer Software in den vergangenen Jahren gefundenen Fehler auf Speicherlecks zurückzuführen, hervorgerufen durch die weitverwendeten unsicheren Sprachen wie C und C++~\cite{Microsoft2019MemoryBugs, RustInAndroid}. Microsoft, SpaceX, Google, Amazon AWS und viele andere Unternehmen fingen bereits an, Rust in ihren Produkten zur erhöhten Sicherheit zu verwenden~\cite{MicrosoftJoinsRust, AmazonLovesRust, RustInAndroid, GoogleRustFoundation}. 

\todo{Ein sanfter Wechsel zur Testgenerierung wäre angebracht.} Software muss getestet werden, um möglichst früh Fehler zu finden, bevor das Produkt beim Endnutzer landet. Allerdings ist das Schreiben der Tests oft langwierig, kostet viel Zeit und mögliche Ausführungsfälle können von Testern vor allem in sehr komplexen Programmen übersehen werden. Aus diesem Grund beschäftigt sich die Forschung seit Jahren mit der Automatisierung dises Prozesses. 

\section{Background}
\subsection{Test Generation in General}
Testsgenerierung ist ein aktiv erforschtes Feld in der Wissenschaft. Im Idealfall kann für ein Programm eine Testsuite von Unit Tests generiert werden, die alle möglichen Pfade im SUT abdecken und gleichzeitig die Korrektheit der Ausführung jedes einzelnen Pfades durch automatische Orakel überprüft, beispielsweise durch Assertions. Ein Orakel ist ein Mechanismus zum Überprüfen, ob ein Output bei einem gegebenen Input richtig ist, beispielsweise mit Hilfe einer formalen Spezifikation~\cite{10.1145/1569901.1570127}. Leider ist das oft nicht möglich. Zum einen können in einem Programm Ausführungspfade existieren, die schlicht unter keinen Umständen erreicht werden können. Zum anderen hat Software nur sehr selten eine formale Spezifikation, die bei der Generierung verwendet werden kann, um Orakel zu generieren. Somit muss in den meisten Fällen ein Entwickler bzw. Tester die generierte Testsuite manuell mit Orakeln versehen (dazu muss er/sie natürlich selbst wissen was das richtige Verhalten ist)~\cite{Fraser_2013}. Dazu muss die generierte Testsuite aber auch möglichst klein und für den Menschen verständlich gehalten werden. 

\subsection{Automatically Generated Oracles}
Davis and Weyuker~\cite{10.1145/800175.809889} haben den Begriff non-testable programs einfegührt, der solche Programme einschließt, für die es keinen Testorakel gibt oder ein Testorakel praktisch nicht umsetzbar ist, und man somit das Ergebnis der Berechnung nicht auf Korrektheit überprüfen kann. Dazu gehören Programme, die entweder erstellt wurden, um das Ergbenis überhaupt zu erfahren, oder Programme, die zu viele Ergebnisse liefern, um sie alle zu überprüfen, oder der Entwickler hatte die Spezifikation missverstanden. Um das Problem eines fehlenden Orakels zu lösen, führten die Autoren einen sogenannten Pseudoorakel ein. Ein Pseudoorakel ist ein zweites, unabhängig implementiertes Programm, das derselben Spezifikation entsprechen muss. Es ist wichtig, dass die zwei Programme von separaten Teams ohne Zwischenkommunikation erstellt werden, damit keine Missverständnisse von einem in das andere Team propagieren können. Anschließend können die Ergebnisse der Berechnungen des originalen Programms und des Pseudoorakels verglichen und es kann über die Validität entschieden werden. 

Das manuelle Erstellen von Pseudoorakeln ist sehr mühselig und ist im Kontext von Testgenerierung für große Projekte nicht lohnenswert. Harman et al.~\cite{1265732} haben das Prinzip der Testability Transformations vorgestellt. Diese sind Quelltext-zu-Quelltext Transformationen, die zum Verbessern der Performanz verschiedener Testgenerierungstechniken führen sollen. McMinn~\cite{10.1145/1569901.1570127} hat diese Idee aufgegriffen und vorgeschlagen, Pseudoorakel für ein gegebenes Programm automatish zu generieren. Er wendet Testability Transformations an, um das originale Programm zu verändern und eine zweite Version zu generieren, die scheinbar gleiche Ausgaben wie die originale haben sollte, es jedoch zu Diskrepanzen kommen kann. 

\subsection{Random Testing}
\subsection{Single-objective Search-based Techniques}
\subsection{Multi/Many-objective Search-based Techniques}
Fraser und Arcuri~\cite{Fraser_2013} haben einen neuen Ansatz für die Generierung von Tests basierend auf der Branch-Abdeckung vorgeschlagen, names Whole Test Suite Generation. Objectives, also in dem Fall Branch Coverage Goals, sind in einem Programm oft von einander abhängig. Viele Branches erfordern, dass andere Branches zuvor in der Ausführung gedeckt werden. Aus diesem Grund stellen die Autoren einen anderen Ansatz, bei dem versucht wird, durch generierte Tests alle Branch Goals auf einmal abzudecken. 

Der MOSA Algorithmus, auch als Many-Objective Sorting Algorithm~\cite{Panichella_2015} bekannt, ist ein mehr oder weniger innovativer Algorithmus, der unter anderem auch auf die Entwicklung von Fraser und Arcuri~\cite{Fraser_2013} setzt. 
\subsection{Dynamic Symbolic Execution}

\section{State of the Art}
\subsection{Fuzzer for Rust}
\subsection{Test Generators for Rust}

\section{Search-based Unit Test Generation in Rust}
\subsection{Testability Transformations}
\subsection{Instrumentation}


\section{Evaluation}
\subsection{Setup}
\subsection{Threats to Validity}
\subsection{Code Coverage Comparison with Manually Written Tests}
\subsection{Code Coverage Comparison with Other Tools}


\section{Conclusion}

\section{Future Work}

\section{Appendix}




%\newpage  % End of first page: you can remove this if you do not need it

%\section{Critical questions}
%\paragraph{Q1:} Is there a way to define some kind of coverage of the generated scenarios?
%\paragraph{Q2:} While some real accidents might be somewhat similar to each other, the %abstract police reports written in natural language boost the similarity of those further. How could the generated test scenarios be limited only to unique ones?

% Remember to list at not less than 4 related work, you can list more if you wish
% If you like you can also use bib files (see below)


\bibliographystyle{unsrt}
\bibliography{bibliography}
%\nocite*{}
\end{document}